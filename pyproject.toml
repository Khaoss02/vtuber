[project]
name = "open-llm-vtuber"
version = "1.1.4"
description = "Talk to any LLM with hands‑free voice interaction, voice interruption, and Live2D running locally across platforms"
readme = "README.md"
requires-python = ">=3.10,<3.13"

dependencies = [
    # Infra / Web
    "fastapi[standard]>=0.115.9",
    "uvicorn[standard]>=0.33.0",
    "aiofiles",
    "python-dotenv",
    "requests>=2.32.3",
    "httpx>=0.28.1",
    "starlette",
    "coloredlogs",
    "loguru>=0.7.2",
    "tomli>=2.2.1",
    "uv",
    "jinja2",

    # Indexing & Embeddings
    "llama-index-core==0.12.42",  # versión compatible con open-llm-vtuber 1.1.4
    "llama-index-readers-file",

    # LLM local (ollama backend usa llama-index + llama.cpp)
    "torch>=2.2.0",
    "torchaudio",
    "transformers>=4.47.0,<4.52.0",  # compatible con coqui (pero si no usas coqui, podrías subir)
    "accelerate",
    "bitsandbytes",
    "sentencepiece",
    "safetensors",
    "huggingface-hub",

    # Multimedia / Audio
    "pillow",
    "soundfile>=0.12.1",
    "pyaudio",
    "pyAudioAnalysis",
    "librosa",
    "silero-vad>=5.1.2",
    "phonemizer",
    "edge-tts>=7.0.0",
    "azure-cognitiveservices-speech>=1.41.1",

    # CV / Viseme / FER
    "opencv-python-headless",
    "fer",

    # Maths / SciPy stack
    "numpy>=1.26.4,<2",
    "scipy>=1.14.1",
    "scikit-learn",
    "sympy",

    # Database & Vector
    "pgvector",
    "psycopg2-binary",
    "chromadb[duckdb]>=0.5.0",
    "duckdb>=0.8.0,<0.9.0",

    # Observabilidad
    "opentelemetry-api",
    "opentelemetry-sdk",
    "opentelemetry-exporter-otlp-proto-grpc",
    "opentelemetry-instrumentation-fastapi",

    # Misc utils
    "bcrypt",
    "cffi",
    "filelock",
    "flatbuffers",
    "fsspec",
    "identify",
    "importlib-resources",
    "mmh3",
    "networkx",
    "nltk",
    "protobuf",
    "pyyaml>=6.0.2",
    "ruamel-yaml>=0.18.10",
    "ruamel-yaml-clib",
    "tabulate",
    "tokenizers",
    "tqdm>=4.67.1",
    "tzdata",
    "typer",
    "win32-setctime",
    "pypiwin32",

    # Conectores externos (ajusta si no usas alguno)
    "py-cord",
    "slack_sdk",
    "twitchio",
    "google-api-python-client",
    "oauth2client",

    # Pre‑commit / lint
    "ruff>=0.8.6",
    "pre-commit>=4.1.0",

    # Dependencias necesarias para LLM Ollama y Sherpa
    "requests",  # para comunicación web
    "sherpa-onnx==1.12.1",  # para ASR sherpa_onnx_asr
]
